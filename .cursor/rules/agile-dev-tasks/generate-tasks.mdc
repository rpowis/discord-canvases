---
description: Agile task generation prioritizing working software every iteration, vertical slices, and demoable progress
globs:
alwaysApply: false
---
# Rule: Generating a Task List from a PRD (Agile)

## Goal

To guide an AI assistant in creating a detailed, step-by-step task list in Markdown format based on an existing Product Requirements Document (PRD). The task list should guide a developer through implementation and ensure each iteration yields working software.

## Output

- **Format:** Markdown (`.md`)
- **Location:** `/tasks/`
- **Filename:** `tasks-[prd-file-name].md` (e.g., `tasks-prd-user-profile-editing.md`)

## Process

1.  **Receive PRD Reference:** The user points the AI to a specific PRD file
2.  **Analyze PRD:** The AI reads and analyzes the functional requirements, user stories, and other sections of the specified PRD.
3.  **Phase 1: Generate Parent Tasks:** Based on the PRD analysis, create the file and generate the main, high-level tasks required to implement the feature. Use your judgement on how many high-level tasks to use. It's likely to be about 5. Present these tasks to the user in the specified format (without sub-tasks yet). Inform the user: "I have generated the high-level tasks based on the PRD. Ready to generate the sub-tasks? Respond with 'Go' to proceed."
4.  **Wait for Confirmation:** Pause and wait for the user to respond with "Go".
5.  **Phase 2: Generate Sub-Tasks:** Once the user confirms, break down each parent task into smaller, actionable sub-tasks necessary to complete the parent task. Ensure sub-tasks logically follow from the parent task and cover the implementation details implied by the PRD. Prefer the smallest demoable increments. Where an external integration or dependency is time‑consuming or costly to set up for this iteration, generate a mock‑based sub‑task now and an explicit follow‑up sub‑task to replace it with the real integration later.
6.  **Identify Relevant Files:** Based on the tasks and PRD, identify potential files that will need to be created or modified. List these under the `Relevant Files` section, including corresponding test files if applicable. Mark file status as (CREATED/UPDATED/MOCKED/REAL).
7.  **Generate Final Output:** Combine the parent tasks, sub-tasks, relevant files, and notes into the final Markdown structure.
8.  **Save Task List:** Save the generated document in the `/tasks/` directory with the filename `tasks-[prd-file-name].md`, where `[prd-file-name]` matches the base name of the input PRD file (e.g., if the input was `prd-user-profile-editing.md`, the output is `tasks-prd-user-profile-editing.md`).

## Output Format

The generated task list must follow this structure:

```markdown
## Relevant Files

- `path/to/potential/file1.ts` - Brief description of why this file is relevant (e.g., Contains the main component for this feature). Status: CREATED/UPDATED/MOCKED/REAL
- `path/to/file1.test.ts` - Unit tests for `file1.ts`.
- `path/to/another/file.tsx` - Brief description (e.g., API route handler for data submission).
- `path/to/another/file.test.tsx` - Unit tests for `another/file.tsx`.
- `lib/utils/helpers.ts` - Brief description (e.g., Utility functions needed for calculations).
- `lib/utils/helpers.test.ts` - Unit tests for `helpers.ts`.

### Notes

- Unit tests should typically be placed alongside the code files they are testing (e.g., `MyComponent.tsx` and `MyComponent.test.tsx` in the same directory).
- Use `npm run test [optional/path/to/test/file]` to run tests. Running without a path executes all tests found by the Jest configuration.
- Prefer the smallest vertical change that demonstrates value. For integrations that are time‑consuming or costly to set up in this iteration, note the mock approach in Technical Notes and add a follow‑up task to replace it with the real integration later.
- Each task must include Acceptance Criteria with explicit Demo Steps (how a reviewer verifies the behavior). Demo Steps must be locally executable and unambiguous (commands, inputs, expected outputs). Prefer a single smoke command if feasible.
- Acceptance Criteria should reference PRD Functional Requirement IDs where applicable (e.g., FR‑3).
- Include Technical Notes only as needed. If applicable, note Mocks/Flags and a simple Rollback/Toggle plan. Default to shipping user‑visible changes behind a feature flag/guard unless explicitly harmless.
- When using a mock: annotate code near the boundary with `TODO: REPLACE_MOCK <task-id>`, add a follow‑up sub‑task linking to the mock location(s), and include demo steps that exercise the mocked path.
- Testing expectations: for mocked paths include at least a smoke test; for real integrations include at least one integration test, or add a clearly scoped Test Debt sub‑task.
- Relevant Files must list both implementation and test files with status. If a file is MOCKED, specify the intended real counterpart or interface boundary.
- (Optional) Performance/Security: if PRD defines SLI/PII constraints, add an acceptance bullet (e.g., `p95 < 300ms for /foo`) or a follow‑up monitoring/task.
- Estimation is optional; infra/exploratory tasks should use a timebox (e.g., "2h timebox; outcome X").
- Definition of Done: build/tests pass, lint clean; demo steps verified by the implementer; task list updated (checkboxes, file statuses); feature flag/toggle documented.
- Ensure tasks are independently mergeable (guard/feature flag when applicable).

## Tasks

- [ ] 1.0 Parent Task Title
  - [ ] 1.1 [Sub-task description 1.1]
  - [ ] 1.2 [Sub-task description 1.2]
- [ ] 2.0 Parent Task Title
  - [ ] 2.1 [Sub-task description 2.1]
- [ ] 3.0 Parent Task Title (may not require sub-tasks if purely structural or configuration)
```

## Interaction Model

The process explicitly requires a pause after generating parent tasks to get user confirmation ("Go") before proceeding to generate the detailed sub-tasks. This ensures the high-level plan aligns with user expectations before diving into details.

## Target Audience

Assume the primary reader of the task list is a **junior developer** who will implement the feature. Ensure each parent task or every 1–2 sub-tasks includes user-visible Demo Steps in acceptance criteria, and note any Mocks/Flags/Toggle considerations in Technical Notes.